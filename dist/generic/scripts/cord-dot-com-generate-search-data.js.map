{
  "version": 3,
  "sources": ["scripts/cord-dot-com-generate-search-data.ts", "server/src/config/MagicEnv.ts", "server/src/config/Env.ts", "docs/lib/parseDownToPlaintext.ts", "docs/lib/hasOwnProperty.ts"],
  "sourcesContent": ["#!/usr/bin/env -S node --enable-source-maps\nimport { writeFileSync } from 'fs';\nimport * as path from 'path';\nimport * as url from 'url';\nimport OpenAI from 'openai';\n\nimport 'dotenv/config.js';\n\nimport Env from 'server/src/config/Env.ts';\nimport type { CordDotComCachedEmbedding } from 'common/types/index.ts';\nimport parseDownToPlaintextStrings from 'docs/lib/parseDownToPlaintext.ts';\n\nconst PAGES_TO_IGNORE = new Set(['https://cord.com/server']);\n\nconst MAX_EMBEDDING_TEXT_LENGTH = 20000;\n\nconst openai = new OpenAI({\n  apiKey: Env.OPENAI_API_SECRET,\n});\n\nasync function createEmbedding(input: string) {\n  return await openai.embeddings.create({\n    model: 'text-embedding-ada-002',\n    input,\n  });\n}\n\nfunction getTitleByFuglyRegex(txt: string): string {\n  const titleTag = txt.match(/<title>([^<]*)<\\/title>/);\n  if (!titleTag) {\n    throw new Error('Could not find any loc tags in the sitemap?');\n  }\n\n  return titleTag[1];\n}\n\nasync function getChunkedSitePages(): Promise<\n  { url: string; title: string; plaintext: string }[]\n> {\n  const res = await fetch('https://cord.com/sitemap.xml');\n\n  const rawXML = await res.text();\n\n  // Yes, I could tediously parse and traverse the XML with a proper XML parser.\n  // Also I'm going to die some day and I that's not an accolade I want to etch\n  // in my tombstone.  So instead, I'm just dumb-parsing it and moving on.\n  const locTags = rawXML.match(/<loc>[^<]*<\\/loc>/g);\n  if (!locTags) {\n    throw new Error('Could not find any loc tags in the sitemap?');\n  }\n  const locs = locTags\n    .map((locTag) => locTag.substring(5, locTag.length - 6))\n    .filter((loc) => !PAGES_TO_IGNORE.has(loc));\n\n  const chunks: { url: string; title: string; plaintext: string }[] = [];\n  const promises: Promise<void>[] = [];\n  for (const locUrl of locs) {\n    promises.push(\n      (async () => {\n        const pageResponse = await fetch(locUrl);\n        const txt = await pageResponse.text();\n        const plaintexts = parseDownToPlaintextStrings(txt);\n        let title = getTitleByFuglyRegex(txt) || 'Cord.com';\n        if (title.includes('Cord | Make the internet multiplayer | ')) {\n          title = title.replace('Cord | Make the internet multiplayer | ', '');\n        }\n        for (const plaintext of plaintexts) {\n          chunks.push({\n            url: locUrl,\n            title,\n            plaintext,\n          });\n        }\n      })(),\n    );\n  }\n  await Promise.all(promises);\n  return chunks;\n}\n\nconst main = async () => {\n  const chunks = await getChunkedSitePages();\n  const embeddings: CordDotComCachedEmbedding[] = [];\n  const promises: Array<Promise<void>> = [];\n  for (const chunk of chunks) {\n    const embedding: CordDotComCachedEmbedding = {\n      url: chunk.url,\n      embedding: undefined,\n      plaintext: chunk.plaintext,\n      title: chunk.title,\n    };\n    embeddings.push(embedding);\n    promises.push(\n      (async () => {\n        if (chunk.plaintext.length > MAX_EMBEDDING_TEXT_LENGTH) {\n          console.error(\n            'Truncating very long plaintext chunk for page: ' + chunk.url,\n          );\n          console.error('Plaintext chunk is: ' + chunk.plaintext);\n          process.exit(1);\n        }\n        try {\n          const data = await createEmbedding(chunk.plaintext);\n          embedding.embedding = data;\n        } catch (e) {\n          console.error(\n            'Failed to fetch embedding for chunk: ' + chunk.plaintext,\n          );\n          console.error((e as Error).message);\n        }\n      })(),\n    );\n  }\n\n  await Promise.all(promises);\n\n  const embeddingsFile = `// @generated by scripts/cord-dot-com-generate-search-data.ts\nimport type { CordDotComCachedEmbedding } from 'common/types/index.ts';\n\nconst embeddings: CordDotComCachedEmbedding[] = ${JSON.stringify(\n    embeddings,\n    null,\n    2,\n  )};\n\nexport default embeddings;\\n`;\n  writeFileSync(\n    path.join(\n      path.dirname(url.fileURLToPath(import.meta.url)),\n      '../../docs/server/searchData/CordDotComEmbeddings.ts',\n    ),\n    embeddingsFile,\n  );\n};\n\nPromise.resolve(main()).catch((err) => {\n  console.error(err);\n  process.exit(1);\n});\n", "// There is some TypeScript trickery in this file. It is optimised for making\n// the use of `magicEnv` look good. The call to `magicEnv` should look readable\n// and self-explanatory, and the type hints displayed by the IDE should be\n// useful.\n\n// Define types to declare variables as required, optional or having a default\n// value.  If these classes were empty, TypeScript would treat them as\n// interchangeable. By giving them different shapes (i.e. different members),\n// TypeScript will keep them apart.\nclass RequiredVariable {\n  public readonly req = true;\n}\nclass OptionalVariable {\n  public readonly opt = true;\n}\nclass VariableWithDefaultValue {\n  constructor(public readonly defaultValue: string) {}\n}\n\n// These are the helpers that are used by the caller of `magicEnv` to define\n// their environment fields.\nexport const required = new RequiredVariable();\nexport const optional = new OptionalVariable();\n// eslint-disable-next-line @typescript-eslint/no-shadow -- Disabling for pre-existing problems. Please do not copy this comment, and consider fixing this one!\nexport const defaultValue = (defaultValue: string) =>\n  new VariableWithDefaultValue(defaultValue);\n\n// Here comes the main function of this module: `magicEnv`. It takes one\n// parameter: a JavaScript object with string keys and values of type\n// RequiredVariable, OptionalVariable or VariableWithDefaultValue.\n//\n// `magicEnv` is a template function, which is quite important.\n// `EnvDefinitionType` is the specific type of the environment definition.\n// That type must comply with the restriction that it is an object with string\n// keys and those variable types as values.  However, we will need the\n// *specific* type, i.e. a TypeScript type that contains the specific keys with\n// the corresponding value types. We get access to this type by templating this\n// function.\nexport function magicEnv<\n  EnvDefinitionType extends {\n    [key: string]:\n      | RequiredVariable\n      | OptionalVariable\n      | VariableWithDefaultValue;\n  },\n>(\n  processEnv: { [key: string]: string | undefined },\n  envDefinition: EnvDefinitionType,\n) {\n  // Now start constructing the result of this function.\n  const env: Partial<{ [k in keyof EnvDefinitionType]: string }> = {};\n\n  // And now we iterate through the `envDefinition` object, which we received\n  // from the callback function.\n  for (const key of Object.keys(envDefinition) as (string &\n    keyof EnvDefinitionType)[]) {\n    // This is the value from the process environment\n    const value: string | undefined = processEnv[key];\n\n    // This is the value from the definition object at the top\n    const fieldDefinition:\n      | RequiredVariable\n      | OptionalVariable\n      | VariableWithDefaultValue = envDefinition[key];\n\n    if ((fieldDefinition as any).req) {\n      // This is a required variable.\n\n      if (value === undefined) {\n        throw new Error(`Missing key ${key} in environment`);\n      } else {\n        env[key] = value;\n      }\n    } else if ((fieldDefinition as any).opt) {\n      // This is an optional variable. `value` may be a string or undefined.\n\n      env[key] = value;\n    } else {\n      // This is a variable with a default value (the value of\n      // `fieldDefinition`)\n\n      if (value === undefined) {\n        env[key] = (fieldDefinition as VariableWithDefaultValue).defaultValue;\n      } else {\n        env[key] = value;\n      }\n    }\n  }\n\n  // Return the `env` object that we have just constructed, but return with a\n  // special type that we define here: it is an object which contains all the\n  // keys that the definition object has. The value type is `string`, except\n  // for fields that correspond to optional variables, those have type\n  // `string | undefined`. All fields are declared readonly.\n  return env as {\n    readonly [k in keyof EnvDefinitionType]: EnvDefinitionType[k] extends OptionalVariable\n      ? string | undefined\n      : string;\n  };\n}\n", "import {\n  magicEnv,\n  required,\n  optional,\n  defaultValue,\n} from 'server/src/config/MagicEnv.ts';\n\nexport default magicEnv(process.env, {\n  // `process.env.NODE_ENV` is used in our code, but also in many third party\n  // libraries we import, to switch between development and production mode.\n  // Just to make sure it is set in the process environment, we include it here.\n  NODE_ENV: required,\n\n  // Normally one of `prod`, `staging`, `dev`, `test`, or `loadtest`\n  CORD_TIER: required,\n\n  // Accept connections on these ports\n  API_SERVER_PORT: optional,\n  ADMIN_SERVER_PORT: optional,\n  METRICS_SERVER_PORT: optional,\n  STATUS_SERVER_PORT: optional,\n  CONSOLE_SERVER_PORT: optional,\n  DOCS_SERVER_PORT: optional,\n\n  // PostgreSQL connection configuration - required\n  POSTGRES_HOST: required,\n  POSTGRES_PORT: required,\n  POSTGRES_USER: required,\n  POSTGRES_PASSWORD: required,\n  POSTGRES_DB: required,\n\n  // PostgreSQL read-only server, if there is one (user/password/db setting same\n  // as above)\n  POSTGRES_READ_HOST: optional,\n  POSTGRES_READ_PORT: optional,\n\n  // Redis connection configuration\n  REDIS_PORT: required,\n  REDIS_HOST: required,\n  PREDIS_PORT: required,\n  PREDIS_HOST: required,\n\n  // URLs pointing to our own endpoints\n  TOP_SERVER_HOST: required,\n  APP_SERVER_HOST: required,\n  API_SERVER_HOST: required,\n  API_SERVER_HOST_PRODUCTION: required,\n  ADMIN_SERVER_HOST: required,\n  MARKETING_SERVER_HOST: required,\n  PUBLIC_UPLOADS_HOST: required,\n  CONSOLE_SERVER_HOST: required,\n  CORD_TO_HOST: required,\n  DOCS_SERVER_HOST: required,\n  CLACK_SERVER_HOST: optional,\n  COMMUNITY_SERVER_HOST: required,\n\n  // Slack App credentials - required\n  SLACK_APP_CLIENT_SECRET: required,\n  SLACK_DEV_APP_CLIENT_SECRET: required,\n  SLACK_ADMIN_LOGIN_REDIRECT_HOST: optional,\n  SLACK_APP_REDIRECT_HOST: optional,\n  SLACK_SIGNING_SECRET: required,\n  SLACK_ADMIN_CLIENT_SECRET: required,\n  SLACK_ADMIN_SIGNING_SECRET: required,\n  SLACK_INTERNAL_BOT_TOKEN: required,\n  SLACK_INTERNAL_SIGNING_SECRET: required,\n  SLACK_CUSTOMER_UPDATES_BOT_TOKEN: required,\n\n  // S3 Bucket File storage\n  S3_ACCESS_KEY_ID: optional,\n  S3_ACCESS_KEY_SECRET: optional,\n  S3_REGION: required,\n  S3_BUCKET: required,\n  S3_PUBLIC_BUCKET: required,\n  S3_ENDPOINT: required,\n  S3_USE_PATH_BASED_URLS: required,\n\n  EMAIL_LINKS_TOKEN_SECRET: required,\n\n  // Jira App credentials - required\n  JIRA_APP_CLIENT_ID: required,\n  JIRA_APP_CLIENT_SECRET: required,\n\n  // Asana App credentials - required\n  ASANA_APP_CLIENT_ID: required,\n  ASANA_APP_CLIENT_SECRET: required,\n\n  // Linear App credentials - required\n  LINEAR_APP_CLIENT_ID: required,\n  LINEAR_APP_CLIENT_SECRET: required,\n\n  // Trello App credentials - required\n  TRELLO_APP_CLIENT_ID: required,\n  TRELLO_APP_CLIENT_SECRET: required,\n\n  // Monday App credentials - required\n  MONDAY_APP_CLIENT_ID: required,\n  MONDAY_APP_CLIENT_SECRET: required,\n\n  // Secret for session tokens - required\n  JWT_SIGNING_SECRET: required,\n\n  // Secret for signing OAuth flow state variables encoding the user and org IDs\n  OAUTH_STATE_SIGNING_SECRET: required,\n\n  // Secret for signing Slack OAuth flow state variables encoding the user and org IDs\n  SLACK_OAUTH_STATE_SIGNING_SECRET: required,\n\n  // Log level for console logging - optional ('info' if not provided)\n  LOGLEVEL: defaultValue('info'),\n\n  // Post error messages to this Slack channel,\n  CORD_OPS_SLACK_CHANNEL_ID: optional,\n\n  // Post informational changes to prod setup, including deploy and db migration\n  // messages, to this Slack channel\n  PROD_CHANGES_SLACK_CHANNEL_ID: optional,\n\n  // Post security/SOC2 compliance messages to this Slack channel,\n  CORD_SECURITY_SLACK_CHANNEL_ID: optional,\n\n  // Post info about go redirects to this Slack channel\n  CORD_GO_REDIRECTS_SLACK_CHANNEL_ID: optional,\n\n  // Post messages from customers to this Slack channel\n  CORD_ALL_CUSTOMERS_SLACK_CHANNEL_ID: optional,\n\n  // Post client request messages to this Slack channel\n  CORD_CLIENT_REQUESTS_SLACK_CHANNEL_ID: optional,\n\n  // For sending search queries from the docs site to Slack\n  CORD_DOCS_SEARCH_SLACK_CHANNEL_ID: optional,\n\n  // Cloudwatch config - optional. If these are not provided, Cloudwatch\n  // logging is disabled\n  CLOUDWATCH_LOGLEVEL: optional,\n  CLOUDWATCH_AWS_REGION: defaultValue('eu-west-2'), // our default region, London\n  CLOUDWATCH_LOG_GROUP_NAME: optional,\n  CLOUDWATCH_LOG_STREAM_NAME: optional,\n\n  // Host used when developing locally but an externally accessible url is\n  // needed. (example: d92dd1d1fa99.ngrok.io)\n  EXTERNAL_API_HOST_FOR_DEVELOPMENT: optional,\n\n  // path to static files for the admin app\n  ADMIN_SERVER_STATIC_PATH: defaultValue('dist/server/admin'),\n\n  // path to static files for the console app\n  CONSOLE_SERVER_STATIC_PATH: defaultValue('dist/server/console'),\n\n  // path to static files for the docs app\n  DOCS_SERVER_STATIC_PATH: defaultValue('dist/docs/static'),\n\n  // API key used to send transactional email notifications through Sendgrid.\n  SENDGRID_API_KEY: required,\n\n  // HTTP Basic Auth name and password for SendGrid's Inbound Parse webhooks\n  SENDGRID_INBOUND_WEBHOOK_USER: required,\n  SENDGRID_INBOUND_WEBHOOK_PASSWORD: required,\n\n  // API key used to fetch feature flags from LaunchDarkly\n  LAUNCHDARKLY_API_KEY: optional,\n\n  // Number of extra workers to run the api server in: 'auto' makes one per CPU; otherwise a specific number can be used\n  NUM_WORKERS: optional,\n\n  // Encryption key used when storing secrets in the database\n  PLATFORM_SECRETS_ENCRYPTION_KEY: required,\n\n  // Encryption key used when generating file permalinks\n  FILE_PROXY_SIGNING_SECRET_KEY: required,\n\n  // sentry.io environment setting\n  SENTRY_ENVIRONMENT: optional,\n  SENTRY_RELEASE: optional,\n  SENTRY_TRACE_SAMPLE_RATE: optional,\n\n  // Auth0 Environment variables\n  // 1) For SPA application\n  AUTH0_CLIENT_ID: required,\n  AUTH0_CUSTOM_LOGIN_DOMAIN: required,\n\n  // 2) For verifying incoming events\n  AUTH0_WEBHOOK_SECRET: required,\n\n  // 3) For server to server communication\n  AUTH0_MTM_CLIENT_ID: required,\n  AUTH0_MTM_CLIENT_SECRET: required,\n  AUTH0_GENERAL_DOMAIN: required,\n\n  // console.cord.com cord app credentials\n  DEV_CONSOLE_CORD_APP_SECRET: required,\n\n  // secret for signing admin tokens to serve as proof user is logged in to\n  // admin\n  ADMIN_TOKEN_SECRET: required,\n\n  // flag whether the SDK testbed should be built and served\n  INCLUDE_SDK_TESTBED: optional,\n\n  // secret for cookies on the docs web site\n  DOCS_COOKIE_PARSER_SECRET: optional,\n\n  // set email for all notifications when testing with users on testbed\n  TESTBED_USERS_EMAIL: optional,\n\n  // secret for creating searchable embeddings and generating search\n  // results within our docs\n  OPENAI_API_SECRET: required,\n\n  // secret for getting geographic information from an IP address\n  IPSTACK_API_SECRET: optional,\n\n  // Host for where we generate the ai chat bot in the docs\n  DOCS_AI_CHATBOT_SERVER_HOST: required,\n\n  // Google analytic events\n  GA_MEASUREMENT_ID: required,\n  GA_MEASUREMENT_PROTOCOL_API_SECRET: required,\n\n  // secret for stripe\n  STRIPE_SECRET_KEY: required,\n  STRIPE_WEBHOOK_SECRET_KEY: required,\n\n  DEMO_APPS_SHARED_SECRET: required,\n\n  // loops.so for sending newletters\n  LOOPS_SO_API_KEY: required,\n});\n", "import { parse } from 'parse5';\nimport type { DefaultTreeAdapterMap } from 'parse5';\nimport { hasOwnProperty } from 'docs/lib/hasOwnProperty.ts';\n\ntype ChildNode = DefaultTreeAdapterMap['childNode'];\ntype TextNode = DefaultTreeAdapterMap['textNode'];\n\n// Set this data attribute to anything truthy in your HTML to have the scraping\n// code ignore it completely.\nconst DATA_SEARCH_IGNORE = 'data-cord-search-ignore';\n\nconst blockLevelNodeNames = new Set([\n  'address',\n  'article',\n  'aside',\n  'blockquote',\n  'canvas',\n  'dd',\n  'div',\n  'dl',\n  'dt',\n  'fieldset',\n  'figcaption',\n  'figure',\n  'footer',\n  'form',\n  'h1',\n  'h2',\n  'h3',\n  'h4',\n  'h5',\n  'h6',\n  'header',\n  'hr',\n  'li',\n  'main',\n  'nav',\n  'noscript',\n  'ol',\n  'p',\n  'pre',\n  'section',\n  'table',\n  'tfoot',\n  'ul',\n  'video',\n]);\n\ntype AttrList = { name: string; value: string }[];\nfunction assertIsAttrList(thing: unknown): AttrList {\n  if (Array.isArray(thing)) {\n    return thing as AttrList;\n  }\n  throw new Error('Malformed attribute list');\n}\n\n// This value is extremely arbitrary. I don't really know enough about LLM\n// to know if many small chunks are better or if we'd do better with more\n// larger ones. This is something worth playing with if we don't feel we're\n// getting useful search results. In my limiting playing with it, smaller\n// chunks seemed to perform worse than larger ones. So, this value is set\n// to push us near the upper limit for the number of tokens that go into\n// an embedding.\nconst MAX_PART_LENGTH = 3900;\n\ntype ParseDownWorkingData = {\n  finalOutput: string[];\n  currentParts: string[];\n  currentPartsLength: number;\n};\n\nfunction addContent(v: string, data: ParseDownWorkingData) {\n  // If the chunk is getting too big, we'll pinch it off here and\n  // reset the counter.\n  if (data.currentPartsLength + v.length > MAX_PART_LENGTH) {\n    data.finalOutput.push(data.currentParts.join(''));\n\n    if (v.length > MAX_PART_LENGTH) {\n      // Tricky case -- one big text node with thousands of character in it\n      let remainder = v;\n      while (remainder.length) {\n        if (remainder.length < MAX_PART_LENGTH) {\n          data.finalOutput.push(remainder);\n          remainder = '';\n          break;\n        }\n\n        // We'll blindly cut into the text in the worst case\n        let sliceIndex = MAX_PART_LENGTH;\n\n        // But we'll also look for a decently placed linebreak and slice there instead;\n        const convenientlyPlacedLineBreakIndex = remainder.indexOf(\n          '\\n',\n          MAX_PART_LENGTH - 200,\n        );\n\n        // And if we don't find a linebreak in a good spot, we'll settle\n        // for a space.\n        const convenientlyPlacedSpaceIndex = remainder.indexOf(\n          ' ',\n          MAX_PART_LENGTH - 200,\n        );\n        if (\n          convenientlyPlacedLineBreakIndex !== -1 &&\n          convenientlyPlacedLineBreakIndex < sliceIndex\n        ) {\n          sliceIndex = convenientlyPlacedLineBreakIndex;\n        } else if (\n          convenientlyPlacedSpaceIndex !== -1 &&\n          convenientlyPlacedSpaceIndex < sliceIndex\n        ) {\n          sliceIndex = convenientlyPlacedSpaceIndex;\n        }\n\n        const slice = remainder.substring(0, sliceIndex);\n        data.finalOutput.push(slice);\n        remainder = remainder.substring(sliceIndex);\n      }\n      data.currentParts = [];\n      data.currentPartsLength = 0;\n    } else {\n      data.currentParts = [v];\n      data.currentPartsLength = v.length;\n    }\n    // Otherwise, just keep tacking the text value onto the current part\n  } else {\n    data.currentParts.push(v);\n    data.currentPartsLength += v.length;\n  }\n}\n\nfunction appendTextNodes(\n  data: ParseDownWorkingData,\n  nodeList: ChildNode[],\n  currentHeadingLevel = 0,\n  isPreformatted = false,\n) {\n  outer: for (const node of nodeList) {\n    if (\n      node.nodeName === 'iframe' ||\n      node.nodeName === 'script' ||\n      node.nodeName === 'noscript' ||\n      node.nodeName === 'style' ||\n      node.nodeName === 'nav' ||\n      node.nodeName === 'header' ||\n      node.nodeName === 'footer' ||\n      node.nodeName === 'wbr'\n    ) {\n      continue;\n    }\n\n    // Try to be a bit smart about chunking together things that are under the\n    // same heading. So, if we encounter an H1 and then an H2, as long as we\n    // have space left in the chunk size, we'll keep appending them into the\n    // same chunk. However, if we've been putting together the content under an\n    // H1 tag and we encounter another H1, we'll pinch off the chunk we've been\n    // working on because the page structure indicates we've got a new topic.\n    //\n    // This works pretty well for pages that have good semantic HTML in the\n    // headings. If the pages have poor semantics in the headings, this is a\n    // dice roll. Garbage in, garbage out. In practice, that will still be okay\n    // because chunks + cosine similarity are surprisingly good at finding the\n    // right needles even when the haystack is hot garbage.\n    let headingLevel = 0;\n    const match = node.nodeName.match(/^h(\\d)$/i);\n    if (match) {\n      headingLevel = parseInt(match[1], 10);\n      if (headingLevel < currentHeadingLevel) {\n        data.finalOutput.push(data.currentParts.join(''));\n        data.currentParts = [];\n        data.currentPartsLength = 0;\n      }\n      currentHeadingLevel = headingLevel;\n    }\n\n    if (node.nodeName === 'hr') {\n      addContent('\\n-----\\n', data);\n    } else if (node.nodeName === '#text') {\n      const v = (node as TextNode).value + (isPreformatted ? '' : '');\n      addContent(v, data);\n    } else {\n      if (hasOwnProperty(node, 'attrs')) {\n        const attrList = assertIsAttrList(node.attrs);\n        for (const attr of attrList) {\n          if (attr.name === DATA_SEARCH_IGNORE) {\n            continue outer;\n          }\n        }\n      }\n      if (\n        hasOwnProperty(node, 'childNodes') &&\n        Array.isArray(node.childNodes)\n      ) {\n        // If we encounter preformatted text, we'll make sure to separate it\n        // from its preceding content.\n        const newPreTag = node.nodeName === 'pre';\n        if (newPreTag) {\n          data.currentParts.push('\\n```\\n');\n          data.currentPartsLength += 5;\n        }\n\n        if (blockLevelNodeNames.has(node.nodeName) && !newPreTag) {\n          data.currentParts.push('\\n');\n          data.currentPartsLength += 1;\n        }\n\n        // This is cheeseball and doesn't work for ordered lists. But it at\n        // least captures that there is a list at all.\n        if (node.nodeName === 'li') {\n          data.currentParts.push('\\n- ');\n          data.currentPartsLength += 3;\n        }\n\n        appendTextNodes(\n          data,\n          node.childNodes,\n          currentHeadingLevel,\n          isPreformatted || newPreTag,\n        );\n\n        if (blockLevelNodeNames.has(node.nodeName) && !newPreTag) {\n          data.currentParts.push('\\n');\n          data.currentPartsLength += 1;\n        }\n\n        if (newPreTag) {\n          data.currentParts.push('\\n```\\n');\n          data.currentPartsLength += 5;\n        }\n      }\n    }\n    if (headingLevel) {\n      data.currentParts.push('\\n\\n');\n      data.currentPartsLength += 2;\n    }\n  }\n}\n\nexport function getBody(childNodes: ChildNode[]): ChildNode | undefined {\n  for (const child of childNodes) {\n    if (child.nodeName === 'body') {\n      return child;\n    } else if (\n      hasOwnProperty(child, 'childNodes') &&\n      Array.isArray(child.childNodes)\n    ) {\n      const body = getBody(child.childNodes as ChildNode[]);\n      if (body) {\n        return body;\n      }\n    }\n  }\n  return undefined;\n}\n\nfunction parseDownToPlaintextStrings(page: string): string[] {\n  const root = parse(page);\n  const body = getBody(root.childNodes);\n  if (\n    !body ||\n    !hasOwnProperty(body, 'childNodes') ||\n    !Array.isArray(body.childNodes)\n  ) {\n    return [];\n  }\n\n  const data: ParseDownWorkingData = {\n    finalOutput: [],\n    currentParts: [],\n    currentPartsLength: 0,\n  };\n  appendTextNodes(data, body.childNodes);\n  if (data.currentParts.length) {\n    const chunk = data.currentParts.join('');\n    data.finalOutput.push(chunk);\n  }\n\n  // Any chunks that are empty strings or just newlines, tabs, and spaces --\n  // nuke them.\n  data.finalOutput = data.finalOutput\n    .map((str) => str.replace(/\\n\\n\\n+/g, '\\n\\n\\n'))\n    .filter((str) => !str.match(/^\\s+$/))\n    .filter(Boolean);\n\n  return data.finalOutput;\n}\n\nexport default parseDownToPlaintextStrings;\n", "// This function determines if X has property Y and does so in a\n// a way that preserves the type information within TypeScript.\nexport function hasOwnProperty<Obj extends object, Prop extends PropertyKey>(\n  obj: Obj,\n  prop: Prop,\n): obj is Obj & { [prop in Prop]: unknown } {\n  return Object.prototype.hasOwnProperty.call(obj, prop);\n}\n"],
  "mappings": ";;;AACA,SAAS,qBAAqB;AAC9B,YAAY,UAAU;AACtB,YAAY,SAAS;AACrB,OAAO,YAAY;AAEnB,OAAO;;;ACGP,IAAM,mBAAN,MAAuB;AAAA,EAAvB;AACE,SAAgB,MAAM;AAAA;AACxB;AACA,IAAM,mBAAN,MAAuB;AAAA,EAAvB;AACE,SAAgB,MAAM;AAAA;AACxB;AACA,IAAM,2BAAN,MAA+B;AAAA,EAC7B,YAA4BA,eAAsB;AAAtB,wBAAAA;AAAA,EAAuB;AACrD;AAIO,IAAM,WAAW,IAAI,iBAAiB;AACtC,IAAM,WAAW,IAAI,iBAAiB;AAEtC,IAAM,eAAe,CAACA,kBAC3B,IAAI,yBAAyBA,aAAY;AAapC,SAAS,SAQd,YACA,eACA;AAEA,QAAM,MAA2D,CAAC;AAIlE,aAAW,OAAO,OAAO,KAAK,aAAa,GACb;AAE5B,UAAM,QAA4B,WAAW,GAAG;AAGhD,UAAM,kBAGyB,cAAc,GAAG;AAEhD,QAAK,gBAAwB,KAAK;AAGhC,UAAI,UAAU,QAAW;AACvB,cAAM,IAAI,MAAM,eAAe,GAAG,iBAAiB;AAAA,MACrD,OAAO;AACL,YAAI,GAAG,IAAI;AAAA,MACb;AAAA,IACF,WAAY,gBAAwB,KAAK;AAGvC,UAAI,GAAG,IAAI;AAAA,IACb,OAAO;AAIL,UAAI,UAAU,QAAW;AACvB,YAAI,GAAG,IAAK,gBAA6C;AAAA,MAC3D,OAAO;AACL,YAAI,GAAG,IAAI;AAAA,MACb;AAAA,IACF;AAAA,EACF;AAOA,SAAO;AAKT;;;AC5FA,IAAO,cAAQ,SAAS,QAAQ,KAAK;AAAA;AAAA;AAAA;AAAA,EAInC,UAAU;AAAA;AAAA,EAGV,WAAW;AAAA;AAAA,EAGX,iBAAiB;AAAA,EACjB,mBAAmB;AAAA,EACnB,qBAAqB;AAAA,EACrB,oBAAoB;AAAA,EACpB,qBAAqB;AAAA,EACrB,kBAAkB;AAAA;AAAA,EAGlB,eAAe;AAAA,EACf,eAAe;AAAA,EACf,eAAe;AAAA,EACf,mBAAmB;AAAA,EACnB,aAAa;AAAA;AAAA;AAAA,EAIb,oBAAoB;AAAA,EACpB,oBAAoB;AAAA;AAAA,EAGpB,YAAY;AAAA,EACZ,YAAY;AAAA,EACZ,aAAa;AAAA,EACb,aAAa;AAAA;AAAA,EAGb,iBAAiB;AAAA,EACjB,iBAAiB;AAAA,EACjB,iBAAiB;AAAA,EACjB,4BAA4B;AAAA,EAC5B,mBAAmB;AAAA,EACnB,uBAAuB;AAAA,EACvB,qBAAqB;AAAA,EACrB,qBAAqB;AAAA,EACrB,cAAc;AAAA,EACd,kBAAkB;AAAA,EAClB,mBAAmB;AAAA,EACnB,uBAAuB;AAAA;AAAA,EAGvB,yBAAyB;AAAA,EACzB,6BAA6B;AAAA,EAC7B,iCAAiC;AAAA,EACjC,yBAAyB;AAAA,EACzB,sBAAsB;AAAA,EACtB,2BAA2B;AAAA,EAC3B,4BAA4B;AAAA,EAC5B,0BAA0B;AAAA,EAC1B,+BAA+B;AAAA,EAC/B,kCAAkC;AAAA;AAAA,EAGlC,kBAAkB;AAAA,EAClB,sBAAsB;AAAA,EACtB,WAAW;AAAA,EACX,WAAW;AAAA,EACX,kBAAkB;AAAA,EAClB,aAAa;AAAA,EACb,wBAAwB;AAAA,EAExB,0BAA0B;AAAA;AAAA,EAG1B,oBAAoB;AAAA,EACpB,wBAAwB;AAAA;AAAA,EAGxB,qBAAqB;AAAA,EACrB,yBAAyB;AAAA;AAAA,EAGzB,sBAAsB;AAAA,EACtB,0BAA0B;AAAA;AAAA,EAG1B,sBAAsB;AAAA,EACtB,0BAA0B;AAAA;AAAA,EAG1B,sBAAsB;AAAA,EACtB,0BAA0B;AAAA;AAAA,EAG1B,oBAAoB;AAAA;AAAA,EAGpB,4BAA4B;AAAA;AAAA,EAG5B,kCAAkC;AAAA;AAAA,EAGlC,UAAU,aAAa,MAAM;AAAA;AAAA,EAG7B,2BAA2B;AAAA;AAAA;AAAA,EAI3B,+BAA+B;AAAA;AAAA,EAG/B,gCAAgC;AAAA;AAAA,EAGhC,oCAAoC;AAAA;AAAA,EAGpC,qCAAqC;AAAA;AAAA,EAGrC,uCAAuC;AAAA;AAAA,EAGvC,mCAAmC;AAAA;AAAA;AAAA,EAInC,qBAAqB;AAAA,EACrB,uBAAuB,aAAa,WAAW;AAAA;AAAA,EAC/C,2BAA2B;AAAA,EAC3B,4BAA4B;AAAA;AAAA;AAAA,EAI5B,mCAAmC;AAAA;AAAA,EAGnC,0BAA0B,aAAa,mBAAmB;AAAA;AAAA,EAG1D,4BAA4B,aAAa,qBAAqB;AAAA;AAAA,EAG9D,yBAAyB,aAAa,kBAAkB;AAAA;AAAA,EAGxD,kBAAkB;AAAA;AAAA,EAGlB,+BAA+B;AAAA,EAC/B,mCAAmC;AAAA;AAAA,EAGnC,sBAAsB;AAAA;AAAA,EAGtB,aAAa;AAAA;AAAA,EAGb,iCAAiC;AAAA;AAAA,EAGjC,+BAA+B;AAAA;AAAA,EAG/B,oBAAoB;AAAA,EACpB,gBAAgB;AAAA,EAChB,0BAA0B;AAAA;AAAA;AAAA,EAI1B,iBAAiB;AAAA,EACjB,2BAA2B;AAAA;AAAA,EAG3B,sBAAsB;AAAA;AAAA,EAGtB,qBAAqB;AAAA,EACrB,yBAAyB;AAAA,EACzB,sBAAsB;AAAA;AAAA,EAGtB,6BAA6B;AAAA;AAAA;AAAA,EAI7B,oBAAoB;AAAA;AAAA,EAGpB,qBAAqB;AAAA;AAAA,EAGrB,2BAA2B;AAAA;AAAA,EAG3B,qBAAqB;AAAA;AAAA;AAAA,EAIrB,mBAAmB;AAAA;AAAA,EAGnB,oBAAoB;AAAA;AAAA,EAGpB,6BAA6B;AAAA;AAAA,EAG7B,mBAAmB;AAAA,EACnB,oCAAoC;AAAA;AAAA,EAGpC,mBAAmB;AAAA,EACnB,2BAA2B;AAAA,EAE3B,yBAAyB;AAAA;AAAA,EAGzB,kBAAkB;AACpB,CAAC;;;ACpOD,SAAS,aAAa;;;ACEf,SAAS,eACd,KACA,MAC0C;AAC1C,SAAO,OAAO,UAAU,eAAe,KAAK,KAAK,IAAI;AACvD;;;ADEA,IAAM,qBAAqB;AAE3B,IAAM,sBAAsB,oBAAI,IAAI;AAAA,EAClC;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AACF,CAAC;AAGD,SAAS,iBAAiB,OAA0B;AAClD,MAAI,MAAM,QAAQ,KAAK,GAAG;AACxB,WAAO;AAAA,EACT;AACA,QAAM,IAAI,MAAM,0BAA0B;AAC5C;AASA,IAAM,kBAAkB;AAQxB,SAAS,WAAW,GAAW,MAA4B;AAGzD,MAAI,KAAK,qBAAqB,EAAE,SAAS,iBAAiB;AACxD,SAAK,YAAY,KAAK,KAAK,aAAa,KAAK,EAAE,CAAC;AAEhD,QAAI,EAAE,SAAS,iBAAiB;AAE9B,UAAI,YAAY;AAChB,aAAO,UAAU,QAAQ;AACvB,YAAI,UAAU,SAAS,iBAAiB;AACtC,eAAK,YAAY,KAAK,SAAS;AAC/B,sBAAY;AACZ;AAAA,QACF;AAGA,YAAI,aAAa;AAGjB,cAAM,mCAAmC,UAAU;AAAA,UACjD;AAAA,UACA,kBAAkB;AAAA,QACpB;AAIA,cAAM,+BAA+B,UAAU;AAAA,UAC7C;AAAA,UACA,kBAAkB;AAAA,QACpB;AACA,YACE,qCAAqC,MACrC,mCAAmC,YACnC;AACA,uBAAa;AAAA,QACf,WACE,iCAAiC,MACjC,+BAA+B,YAC/B;AACA,uBAAa;AAAA,QACf;AAEA,cAAM,QAAQ,UAAU,UAAU,GAAG,UAAU;AAC/C,aAAK,YAAY,KAAK,KAAK;AAC3B,oBAAY,UAAU,UAAU,UAAU;AAAA,MAC5C;AACA,WAAK,eAAe,CAAC;AACrB,WAAK,qBAAqB;AAAA,IAC5B,OAAO;AACL,WAAK,eAAe,CAAC,CAAC;AACtB,WAAK,qBAAqB,EAAE;AAAA,IAC9B;AAAA,EAEF,OAAO;AACL,SAAK,aAAa,KAAK,CAAC;AACxB,SAAK,sBAAsB,EAAE;AAAA,EAC/B;AACF;AAEA,SAAS,gBACP,MACA,UACA,sBAAsB,GACtB,iBAAiB,OACjB;AACA,QAAO,YAAW,QAAQ,UAAU;AAClC,QACE,KAAK,aAAa,YAClB,KAAK,aAAa,YAClB,KAAK,aAAa,cAClB,KAAK,aAAa,WAClB,KAAK,aAAa,SAClB,KAAK,aAAa,YAClB,KAAK,aAAa,YAClB,KAAK,aAAa,OAClB;AACA;AAAA,IACF;AAcA,QAAI,eAAe;AACnB,UAAM,QAAQ,KAAK,SAAS,MAAM,UAAU;AAC5C,QAAI,OAAO;AACT,qBAAe,SAAS,MAAM,CAAC,GAAG,EAAE;AACpC,UAAI,eAAe,qBAAqB;AACtC,aAAK,YAAY,KAAK,KAAK,aAAa,KAAK,EAAE,CAAC;AAChD,aAAK,eAAe,CAAC;AACrB,aAAK,qBAAqB;AAAA,MAC5B;AACA,4BAAsB;AAAA,IACxB;AAEA,QAAI,KAAK,aAAa,MAAM;AAC1B,iBAAW,aAAa,IAAI;AAAA,IAC9B,WAAW,KAAK,aAAa,SAAS;AACpC,YAAM,IAAK,KAAkB,SAAS,iBAAiB,KAAK;AAC5D,iBAAW,GAAG,IAAI;AAAA,IACpB,OAAO;AACL,UAAI,eAAe,MAAM,OAAO,GAAG;AACjC,cAAM,WAAW,iBAAiB,KAAK,KAAK;AAC5C,mBAAW,QAAQ,UAAU;AAC3B,cAAI,KAAK,SAAS,oBAAoB;AACpC,qBAAS;AAAA,UACX;AAAA,QACF;AAAA,MACF;AACA,UACE,eAAe,MAAM,YAAY,KACjC,MAAM,QAAQ,KAAK,UAAU,GAC7B;AAGA,cAAM,YAAY,KAAK,aAAa;AACpC,YAAI,WAAW;AACb,eAAK,aAAa,KAAK,SAAS;AAChC,eAAK,sBAAsB;AAAA,QAC7B;AAEA,YAAI,oBAAoB,IAAI,KAAK,QAAQ,KAAK,CAAC,WAAW;AACxD,eAAK,aAAa,KAAK,IAAI;AAC3B,eAAK,sBAAsB;AAAA,QAC7B;AAIA,YAAI,KAAK,aAAa,MAAM;AAC1B,eAAK,aAAa,KAAK,MAAM;AAC7B,eAAK,sBAAsB;AAAA,QAC7B;AAEA;AAAA,UACE;AAAA,UACA,KAAK;AAAA,UACL;AAAA,UACA,kBAAkB;AAAA,QACpB;AAEA,YAAI,oBAAoB,IAAI,KAAK,QAAQ,KAAK,CAAC,WAAW;AACxD,eAAK,aAAa,KAAK,IAAI;AAC3B,eAAK,sBAAsB;AAAA,QAC7B;AAEA,YAAI,WAAW;AACb,eAAK,aAAa,KAAK,SAAS;AAChC,eAAK,sBAAsB;AAAA,QAC7B;AAAA,MACF;AAAA,IACF;AACA,QAAI,cAAc;AAChB,WAAK,aAAa,KAAK,MAAM;AAC7B,WAAK,sBAAsB;AAAA,IAC7B;AAAA,EACF;AACF;AAEO,SAAS,QAAQ,YAAgD;AACtE,aAAW,SAAS,YAAY;AAC9B,QAAI,MAAM,aAAa,QAAQ;AAC7B,aAAO;AAAA,IACT,WACE,eAAe,OAAO,YAAY,KAClC,MAAM,QAAQ,MAAM,UAAU,GAC9B;AACA,YAAM,OAAO,QAAQ,MAAM,UAAyB;AACpD,UAAI,MAAM;AACR,eAAO;AAAA,MACT;AAAA,IACF;AAAA,EACF;AACA,SAAO;AACT;AAEA,SAAS,4BAA4B,MAAwB;AAC3D,QAAM,OAAO,MAAM,IAAI;AACvB,QAAM,OAAO,QAAQ,KAAK,UAAU;AACpC,MACE,CAAC,QACD,CAAC,eAAe,MAAM,YAAY,KAClC,CAAC,MAAM,QAAQ,KAAK,UAAU,GAC9B;AACA,WAAO,CAAC;AAAA,EACV;AAEA,QAAM,OAA6B;AAAA,IACjC,aAAa,CAAC;AAAA,IACd,cAAc,CAAC;AAAA,IACf,oBAAoB;AAAA,EACtB;AACA,kBAAgB,MAAM,KAAK,UAAU;AACrC,MAAI,KAAK,aAAa,QAAQ;AAC5B,UAAM,QAAQ,KAAK,aAAa,KAAK,EAAE;AACvC,SAAK,YAAY,KAAK,KAAK;AAAA,EAC7B;AAIA,OAAK,cAAc,KAAK,YACrB,IAAI,CAAC,QAAQ,IAAI,QAAQ,YAAY,QAAQ,CAAC,EAC9C,OAAO,CAAC,QAAQ,CAAC,IAAI,MAAM,OAAO,CAAC,EACnC,OAAO,OAAO;AAEjB,SAAO,KAAK;AACd;AAEA,IAAO,+BAAQ;;;AHnRf,IAAM,kBAAkB,oBAAI,IAAI,CAAC,yBAAyB,CAAC;AAE3D,IAAM,4BAA4B;AAElC,IAAM,SAAS,IAAI,OAAO;AAAA,EACxB,QAAQ,YAAI;AACd,CAAC;AAED,eAAe,gBAAgB,OAAe;AAC5C,SAAO,MAAM,OAAO,WAAW,OAAO;AAAA,IACpC,OAAO;AAAA,IACP;AAAA,EACF,CAAC;AACH;AAEA,SAAS,qBAAqB,KAAqB;AACjD,QAAM,WAAW,IAAI,MAAM,yBAAyB;AACpD,MAAI,CAAC,UAAU;AACb,UAAM,IAAI,MAAM,6CAA6C;AAAA,EAC/D;AAEA,SAAO,SAAS,CAAC;AACnB;AAEA,eAAe,sBAEb;AACA,QAAM,MAAM,MAAM,MAAM,8BAA8B;AAEtD,QAAM,SAAS,MAAM,IAAI,KAAK;AAK9B,QAAM,UAAU,OAAO,MAAM,oBAAoB;AACjD,MAAI,CAAC,SAAS;AACZ,UAAM,IAAI,MAAM,6CAA6C;AAAA,EAC/D;AACA,QAAM,OAAO,QACV,IAAI,CAAC,WAAW,OAAO,UAAU,GAAG,OAAO,SAAS,CAAC,CAAC,EACtD,OAAO,CAAC,QAAQ,CAAC,gBAAgB,IAAI,GAAG,CAAC;AAE5C,QAAM,SAA8D,CAAC;AACrE,QAAM,WAA4B,CAAC;AACnC,aAAW,UAAU,MAAM;AACzB,aAAS;AAAA,OACN,YAAY;AACX,cAAM,eAAe,MAAM,MAAM,MAAM;AACvC,cAAM,MAAM,MAAM,aAAa,KAAK;AACpC,cAAM,aAAa,6BAA4B,GAAG;AAClD,YAAI,QAAQ,qBAAqB,GAAG,KAAK;AACzC,YAAI,MAAM,SAAS,yCAAyC,GAAG;AAC7D,kBAAQ,MAAM,QAAQ,2CAA2C,EAAE;AAAA,QACrE;AACA,mBAAW,aAAa,YAAY;AAClC,iBAAO,KAAK;AAAA,YACV,KAAK;AAAA,YACL;AAAA,YACA;AAAA,UACF,CAAC;AAAA,QACH;AAAA,MACF,GAAG;AAAA,IACL;AAAA,EACF;AACA,QAAM,QAAQ,IAAI,QAAQ;AAC1B,SAAO;AACT;AAEA,IAAM,OAAO,YAAY;AACvB,QAAM,SAAS,MAAM,oBAAoB;AACzC,QAAM,aAA0C,CAAC;AACjD,QAAM,WAAiC,CAAC;AACxC,aAAW,SAAS,QAAQ;AAC1B,UAAM,YAAuC;AAAA,MAC3C,KAAK,MAAM;AAAA,MACX,WAAW;AAAA,MACX,WAAW,MAAM;AAAA,MACjB,OAAO,MAAM;AAAA,IACf;AACA,eAAW,KAAK,SAAS;AACzB,aAAS;AAAA,OACN,YAAY;AACX,YAAI,MAAM,UAAU,SAAS,2BAA2B;AACtD,kBAAQ;AAAA,YACN,oDAAoD,MAAM;AAAA,UAC5D;AACA,kBAAQ,MAAM,yBAAyB,MAAM,SAAS;AACtD,kBAAQ,KAAK,CAAC;AAAA,QAChB;AACA,YAAI;AACF,gBAAM,OAAO,MAAM,gBAAgB,MAAM,SAAS;AAClD,oBAAU,YAAY;AAAA,QACxB,SAAS,GAAG;AACV,kBAAQ;AAAA,YACN,0CAA0C,MAAM;AAAA,UAClD;AACA,kBAAQ,MAAO,EAAY,OAAO;AAAA,QACpC;AAAA,MACF,GAAG;AAAA,IACL;AAAA,EACF;AAEA,QAAM,QAAQ,IAAI,QAAQ;AAE1B,QAAM,iBAAiB;AAAA;AAAA;AAAA,kDAGyB,KAAK;AAAA,IACnD;AAAA,IACA;AAAA,IACA;AAAA,EACF,CAAC;AAAA;AAAA;AAAA;AAGD;AAAA,IACO;AAAA,MACE,aAAY,kBAAc,YAAY,GAAG,CAAC;AAAA,MAC/C;AAAA,IACF;AAAA,IACA;AAAA,EACF;AACF;AAEA,QAAQ,QAAQ,KAAK,CAAC,EAAE,MAAM,CAAC,QAAQ;AACrC,UAAQ,MAAM,GAAG;AACjB,UAAQ,KAAK,CAAC;AAChB,CAAC;",
  "names": ["defaultValue"]
}
